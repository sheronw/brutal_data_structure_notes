# Algorithms Analysis

date: 2019-1-5

## big-oh notation

其实大 O 符号的标准定义是这样的：

设 f 和 g 是定义域为自然数集 N 上的函数，若存在正数 c 和 n，使得对一切 n>n<sub>0</sub>，0<=f(n)<=cg(n)成立，则称**f(n)的渐进上界 g(n)，记作 f(n)=O(g(n))**。

……虽然我希望你认真地 阅读以上定义并理解，但如果是非本专业的学生，只是学学数据结构的话，算法分析方面的知识只要下狠心背下来就 ok，这个定义不看也可以的（当然知道这玩意儿是渐进上界倒是能够帮助理解）。

大白话来讲（虽然可能这么说有点差错），big-oh notation 就是一个近似函数，而且是比原函数要大的那种。当然也不要求一直都大，但一定要在某个值之后一定比这个值要大。

大概就是下图这样？不要纠结函数形状，只要 g(n)超过一个值之后一直比 O(g(n))大就行……

![enter image description here](https://i.loli.net/2019/01/06/5c31685b1e670.png)

几个在这个阶段会比较有用的定理：

- if f(n) is O(c\*g(n)) for some constant c, then f(n) is O(g(n))
- (前面的系数是可以忽略的)
- If f1(n) is O(g1(n))and f2(n) is O(g2(n), then f1(n)+f2(n) is O(max(g1(n), g2(n))
- (两个函数相加，对长远趋势起决定性作用的肯定是增的更快的那个，所以忽略更小的函数)
- 对数的底并不重要（有个公式也很好推导，不放了）

## running times for common algorithms

但是，这个 big-oh notation 到底有什么用呢？它是衡量一个程序（或者说，算法）好坏的重要指标。毕竟我们总是希望一个程序需要的内存越少越好，运行的时间越快越好。因为这节课没有涉及内存（空间复杂度）的分析，所以只需要知道如何分析时间就好了。

程序运行的时间其实和运行这个程序的机器还有编译器之类的都有关，但我们暂时不去管它们，那么还有关的就是输入的**数据量**（也就是说 n）还有**算法本身**了。running time 就是一个自变量为 n 的函数，但因为还有别的影响因素，也没有那个成本一个个程序统计精确的函数值（也不需要），所以就用到了近似函数 Big-Oh notation 啦。

当然，同一个算法，由于输入的数据不同，running time 也是不一样的，总的来说，有如下三种：

- best-case running time
- average-case running time
- worst-case running time

默认情况下，都是分析 worst-case running time，毕竟最坏情况下的运行时间也肯定是平均情况的上界……best-case 感觉很少分析，因为好像没用……？

举个例子，最简单粗暴的线性搜索，共有 n 个数据，for 循环从第一个开始一个个查的那种。

- 要是我们人品好，发现第一个就是我们要的，一次就搜到了，best-case running time 就是 O(1)。
- 要是我们运气没那么差，数据的 index 在 1~n-1 之间，那也是小于 n 的某个值，不会超过 n，那么渐进上界仍然是 n，average-case running time 就是 O(n)。
- 要是正好最后一个数据才是我们要找的……就说明我们搜了 n 次，worst-case running time 还是 O(n)。
  那么其他的程序怎么分析呢？

遇到循环体，不管是 for 还是 while 先看看这段程序要运行几次（用 n 表示），然后它就是循环的这部分前面的系数。
而一段程序可以分为多个运行部分的和，按照前面的定理，只要找出最耗时间的那一部分，剩下的全忽略就好。

```c
  printf("WOW");
  //不管输入的数据有多少，运行上面这行代码所需要的都是常数时间，只要是常数时间都算O(1)
  for(int i=0;i<n;i++){//运行了n次
      printf("???");
      //同理O(1)
  }
  //所以是O(1+n*1)=O(n)
```

Binary Search 是 O(logn)，这个显然比刚刚那个快。大概就是给定一堆排好序的数据，看看中间的数据比搜索数据大还是小，这样就可以排除一半的数据，然后以此类推，最坏的情况是直到搜索范围只剩下 1。

因为每次比较搜索数据和数据中的某个数据哪个大都是常数时间，所以只需要找出最坏要搜索多少次就可以了。如果一共有 8 个数据，第一次没搜到，缩小到 4 个数据，还没找到，缩小到 2 个数据……你会发现数据的总数=2^(总搜索次数)，也就是说总搜索次数=log<sub>2</sub>n，也就是 O(logn)。

## recurrence relations

虽然老师上课没这么讲，但我觉得对付递归的最好方法是一个叫做递归树的画图方法。

比如用递归来求斐波那契数列吧（请注意，这是一个非常糟糕的算法）。

伪代码差不多是这样事儿的：

```python
  f(n):
      if n==0 or n==1:
          return n
      else:
          return f(n-1)+f(n-2)
```

首先，要找出来 T(n)和下一层递归的关系。在 n 比较大的情况下，想求 f(n)就要把 f(n-1)和 f(n-2)都算出来，两个都需要时间，而得到这两个值之后相加是常数时间 O(1)，所以关系就是 T(n)=T(n-1)+T(n-2)。

递归树的原理是这样的，先在每一个节点都写上这一层需要运行的时间，然后把这个运行时间拆成一棵树，只属于这个节点的运行时间留下，剩下的放到下一层。每画一个树，三个节点的和都等于没拆分之前单独一个节点的运行时间。

![enter image description here](https://i.loli.net/2019/01/06/5c317ff7b4aa8.png)

这样我们就画出来了一棵树……其实每个分支节点的高度不应该是一样的，但毕竟是渐进上界嘛，这个树差不多有 n 层（最长的那一条应该是一路往左，每次减一），每个节点是 O(1)，那么 T(n)=O(1)+O(2)+O(4)+…+O(2<sup>n</sup>)，按照公式 T(n)=O(2<sup>n</sup>)。

所以这个玩意儿显然是成指数增长的，数一大就会变得巨慢无比（不信可以跑跑看看），不是个好算法……
一般来说，O(1)<O(logn)<O(n)<O(nlogn)<O(n<sup>a</sup>)<O(a<sup>n</sup>)，a 是一个常数
